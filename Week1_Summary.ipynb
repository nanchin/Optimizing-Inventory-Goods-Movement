{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content:\n",
    "    1) Import library\n",
    "    2) Configuration\n",
    "    3) Define Functions\n",
    "    4) Basic Checks\n",
    "    5) Export Data Description\n",
    "    6) Data Cleaning\n",
    "        6.1) Drop columns\n",
    "            6.1.1) Drop empty columns\n",
    "            6.1.2) Drop unilabel/univalue columns\n",
    "        6.2) Transaction\n",
    "            6.2) Fix Data Quality Issue in 'Fruit Size Code' \n",
    "        6.3) Dispatched\n",
    "    7) Convert to Time Series Data\n",
    "        7.1) Transaction\n",
    "            7.1.1) Drop duplicated transactions\n",
    "        7.2) Dispatched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "\n",
    "from scipy.stats import skew,kurtosis\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# to view all columns\n",
    "pd.set_option('display.max_columns',500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_1='C:/Users/Nan/Documents/GitHub_Data/TransactionDetails.csv'\n",
    "filepath_2='C:/Users/Nan/Documents/GitHub_Data/Dispatched.csv'\n",
    "filename_1=filepath_1.rstrip('.csv')\n",
    "filename_2=filepath_2.rstrip('.csv')\n",
    "transaction=pd.read_csv(filepath_1)\n",
    "dispatched=pd.read_csv(filepath_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datatype(arg_df):\n",
    "    \n",
    "    col_bool=[]\n",
    "    col_object=[]\n",
    "    col_number=[]\n",
    "    col_catogory=[]\n",
    "    \n",
    "    for col in arg_df.columns:\n",
    "        datatype=arg_df[col].dtypes\n",
    "        if datatype==bool:\n",
    "            col_bool.append(col)\n",
    "        elif datatype==object :\n",
    "            col_object.append(col)\n",
    "        elif str(datatype)=='category':\n",
    "            col_catogory.append(col)\n",
    "        else:\n",
    "            col_number.append(col)\n",
    "    print('This dataset has {} Columns\\nbool\\t:{} \\nobject\\t:{}  \\ncategory:{} \\nnumeric\\t:{} '\n",
    "          .format(len(arg_df.columns),len(col_bool),len(col_object),len(col_catogory),len(col_number)))\n",
    "    \n",
    "    del arg_df\n",
    "    gc.collect()\n",
    "    \n",
    "    return col_bool,col_object,col_catogory,col_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_object(arg_df):\n",
    "    \n",
    "    object_list=[]\n",
    "    category_list=[]\n",
    "    bool_list=[]\n",
    "    unilabel_list=[]\n",
    "    missing_list=[]\n",
    "    \n",
    "    for c in arg_df.columns:\n",
    "        if arg_df[c].dtypes==object:\n",
    "            object_list.append(c)\n",
    "        elif str(arg_df[c].dtypes)=='category':\n",
    "            category_list.append(c)\n",
    "        elif arg_df[c].dtypes==bool:\n",
    "            bool_list.append(c)\n",
    "    if len(object_list)+len(category_list)+len(bool_list)>0:    \n",
    "        index_list=['Count','Unique','Missing (%)','Top','Top (%)','Bottom','Bottom (%)']\n",
    "        df_summary=pd.DataFrame(data=np.zeros((len(index_list),len(object_list))),index=index_list,columns=object_list)\n",
    "\n",
    "        for col in object_list+category_list+bool_list:\n",
    "            vc=arg_df[col].value_counts().reset_index()\n",
    "            df_summary.loc['Count',col]=(arg_df[col].count())\n",
    "            df_summary.loc['Unique',col]=len(arg_df[col].unique())\n",
    "            df_summary.loc['Missing (%)',col]=arg_df[col].isna().mean()*100\n",
    "            df_summary.loc['Top',col]=vc.iloc[0,0]\n",
    "            df_summary.loc['Top (%)',col]=vc.iloc[0,1]/len(arg_df)*100\n",
    "            if len(arg_df[col].unique())>1:\n",
    "                df_summary.loc['Bottom',col]=vc.iloc[-1,0]\n",
    "                df_summary.loc['Bottom (%)',col]=vc.iloc[-1,1]/len(arg_df)*100\n",
    "            else:\n",
    "                unilabel_list.append(col)\n",
    "            if df_summary.loc['Missing (%)',col]==100:\n",
    "                missing_list.append(col)\n",
    "                \n",
    "        df_summary=df_summary.T.sort_values(['Missing (%)','Unique'],ascending=False)\n",
    "        df_summary=df_summary[(df_summary['Unique']>1) & (df_summary['Missing (%)']!=100)]\n",
    "        df_summary.reset_index(inplace=True)\n",
    "        df_summary.index=df_summary.index+1\n",
    "        df_summary.columns=['Attribute']+index_list\n",
    "        \n",
    "        print('SUMMARY OF {} NON-NUMERICAL ATTRIBUTES:\\n'.format(\n",
    "            len(object_list)+len(category_list)+len(bool_list)))\n",
    "        if len(object_list)>0:\n",
    "            print('{} Object Columns'.format(len(object_list)))\n",
    "        if len(category_list)>0:\n",
    "            print('{} Categorical Columns'.format(len(category_list)))\n",
    "        if len(bool_list)>0:\n",
    "            print('{} Bool Columns'.format(len(bool_list)))\n",
    "        if len(unilabel_list)>0:\n",
    "            print('\\n{} Columns with Single Label : \\n{}'.format(len(unilabel_list),unilabel_list))\n",
    "        if len(missing_list)>0:\n",
    "            print('\\n{} Empty Columns: \\n{}'.format(len(missing_list),missing_list))   \n",
    "            \n",
    "        del arg_df,object_list,vc,index_list,unilabel_list,missing_list\n",
    "        gc.collect()\n",
    "        return df_summary\n",
    "    else:\n",
    "        print('No Non-Numerical Attributes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_numerical(arg_df):\n",
    "\n",
    "    target_list=[]\n",
    "    missing_list=[]\n",
    "    zero_skew_list=[]\n",
    "    \n",
    "    for c in arg_df.columns:\n",
    "        datatype=arg_df[c].dtypes\n",
    "        if datatype != object and datatype != bool and str(datatype) != 'category':\n",
    "            target_list.append(c)\n",
    "    if len(target_list)>0:\n",
    "        from scipy.stats import skew,kurtosis\n",
    "        \n",
    "        index_list=['Count','Missing (%)','Mean','Median','Min','Max','Skewness','Kurtosis']\n",
    "        df_summary=pd.DataFrame(data=np.zeros((len(index_list),len(target_list))),\n",
    "                                index=index_list,columns=target_list)\n",
    "        for col in target_list:\n",
    "            df_summary.loc['Count',col]=arg_df[col].count()\n",
    "            df_summary.loc['Missing (%)',col]=arg_df[col].isna().mean()*100\n",
    "            if df_summary.loc['Missing (%)',col]!=100:\n",
    "                df_summary.loc['Mean',col]=arg_df[col].mean()\n",
    "                df_summary.loc['Median',col]=arg_df[col].median()\n",
    "                df_summary.loc['Min',col]=arg_df[col].min()\n",
    "                df_summary.loc['Max',col]=arg_df[col].max()\n",
    "                df_summary.loc['Skewness',col]=skew(arg_df[col])\n",
    "                if df_summary.loc['Skewness',col]==0:\n",
    "                    zero_skew_list.append(col)\n",
    "                df_summary.loc['Kurtosis',col]=kurtosis(arg_df[col])\n",
    "            else:\n",
    "                missing_list.append(col)\n",
    "                \n",
    "        df_summary=df_summary.T.sort_values(['Missing (%)','Skewness'],ascending=False)\n",
    "        df_summary=df_summary[(df_summary['Skewness']!=0) & (df_summary['Missing (%)']!=100)]\n",
    "        df_summary.reset_index(inplace=True)\n",
    "        df_summary.index=df_summary.index+1\n",
    "        df_summary.columns=['Attribute']+index_list\n",
    "        \n",
    "        print('SUMMARY OF {} NUMERICAL ATTRIBUTES:'.format(len(target_list)))\n",
    "        if len(zero_skew_list)>0:\n",
    "            print('\\n{} Columns with Single Value: \\n{}'.format(len(zero_skew_list),zero_skew_list))\n",
    "        if len(missing_list)>0:\n",
    "            print('\\n{} Empty Columns: \\n{}'.format(len(missing_list),missing_list))\n",
    "        del arg_df,target_list,index_list\n",
    "        gc.collect()\n",
    "\n",
    "        return df_summary\n",
    "    else:\n",
    "        print('No Numerical Attributes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_Data_Description(arg_df,**kwarg):\n",
    "    from scipy.stats import skew\n",
    "    data_description=pd.DataFrame()\n",
    "    for c in arg_df.columns:\n",
    "        data_description.loc[c,'Datatype']=arg_df[c].dtypes\n",
    "        data_description.loc[c,'Missing%']='{:.3f}'.format((len(arg_df[c])-arg_df[c].count())/len(arg_df[c])*100)\n",
    "        if (len(arg_df[c])-arg_df[c].count())/len(arg_df[c])*100!=100:\n",
    "            if arg_df[c].dtypes==object:\n",
    "                data_description.loc[c,'Unique']=len(arg_df[c].unique())\n",
    "                if len(arg_df[c].unique())==1:\n",
    "                    data_description.loc[c,'Remark']='Dropped because this column has only single lable'\n",
    "                else:\n",
    "                    data_description.loc[c,'Remark']='Frequent: {} ({:.3f} %)'.format(\n",
    "                        arg_df[c].mode()[0],arg_df[arg_df[c]==arg_df[c].mode()[0]][c].count()/len(arg_df[c])*100)\n",
    "            else:\n",
    "                if skew(arg_df[c])==0:\n",
    "                    data_description.loc[c,'Unique']=1\n",
    "                    data_description.loc[c,'Remark']='Dropped because this column has only single value'\n",
    "                else:\n",
    "                    data_description.loc[c,'Remark']='MAX: {:.3f} MIN: {:.3f} MEAN: {:.3f} STD: {:.3f}'.format(\n",
    "                        arg_df[c].max(),arg_df[c].min(),arg_df[c].mean(),arg_df[c].std())\n",
    "        else:\n",
    "            data_description.loc[c,'Remark']='Dropped because this column is empty'\n",
    "    data_description.reset_index(inplace=True)\n",
    "    data_description.index=data_description.index+1\n",
    "    data_description=data_description.rename(columns={'index':'Attribute'})\n",
    "    if ('surfix' in kwarg):\n",
    "        data_description.to_excel('data_description_{}.xlsx'.format(kwarg['surfix']))\n",
    "    else:\n",
    "        import datetime\n",
    "        currentDT = datetime.datetime.now()\n",
    "        time=str(currentDT.year)+'-'+str(currentDT.month)+'-'+str(currentDT.day)+' '+str(currentDT.hour)+str(currentDT.minute)+str(currentDT.second)\n",
    "        data_description.to_excel('data_description_{}.xlsx'.format(time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rectify_to_category(arg_df,actual_col_list):\n",
    "    \n",
    "    object_list=[]\n",
    "    '''\n",
    "    for c in arg_df.columns:\n",
    "        if arg_df[c].dtypes==object or str(arg_df[c].dtypes)=='category':\n",
    "            object_list.append(c)\n",
    "    if len(object_list)>0: \n",
    "        for column in [c for c in object_list if c not in actual_col_list]: '''\n",
    "    for c in arg_df.columns:\n",
    "        if arg_df[c].dtype!=bool:\n",
    "            object_list.append(c)\n",
    "    if len(object_list)>0:\n",
    "        for columns in [c for c in object_list if c not in actual_col_list]:\n",
    "            arg_df[columns]=arg_df[columns].astype('category',inplace=True)\n",
    "        print('Change Datatype of {} Column to Category : \\n{}'.format(len(object_list),object_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unilable_column(arg_df):\n",
    "    \n",
    "    target_list=[]\n",
    "    object_list=[]\n",
    "    number_list=[]\n",
    "    for c in arg_df.columns:\n",
    "        if (arg_df[c].dtypes==object) | (str(arg_df[c].dtypes)=='category') | (arg_df[c].dtypes==bool):\n",
    "            object_list.append(c)\n",
    "        else:\n",
    "            number_list.append(c)\n",
    "    if len(object_list)>0:    \n",
    "        for c in object_list:\n",
    "            if len(arg_df[c].unique())==1:\n",
    "                target_list.append(c)\n",
    "    \n",
    "    if len(number_list)>0:   \n",
    "        from scipy.stats import skew\n",
    "        for c in number_list:\n",
    "            if skew(arg_df[c])==0:\n",
    "                target_list.append(c)\n",
    "                \n",
    "    if len(target_list)>0:\n",
    "        arg_df.drop(columns=target_list,axis='columns',inplace=True)\n",
    "        print('Drop {} Columns with Single Label:\\n{}'.format(len(target_list),target_list))\n",
    "    else: \n",
    "        print('No Columns with Single Label/Value')\n",
    "\n",
    "    del target_list,object_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_empty_column(arg_df):\n",
    "    target_list=[]\n",
    "    for c in arg_df.columns:\n",
    "        if arg_df[c].count()==0:\n",
    "            target_list.append(c)\n",
    "    if len(target_list)>0:\n",
    "        arg_df.drop(columns=target_list,axis=1,inplace=True)\n",
    "        print('Delete {} Empty Column : \\n{}'.format(len(target_list),target_list))\n",
    "    else:\n",
    "        print('No Empty Column')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Basic Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1) Dataset -Transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_list,object_list,cat_list,num_list=get_datatype(transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary_object(transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_numerical(transaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2) Dataset - Dispatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispatched.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_list,object_list,cat_list,num_list=get_datatype(dispatched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispatched.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_object(dispatched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_numerical(dispatched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Export Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export_Data_Description(transaction,surfix='transaction')\n",
    "#export_Data_Description(dispatched,surfix='dispatched')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1) Drop columns\n",
    "    6.1.1) Drop empty columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_empty_column(transaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    6.1.2) Drop unilabel/univalue columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_unilable_column(transaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2) Dataset -Transaction\n",
    "    6.2.1) Fix Data Quality Issue in 'Fruit Size Code' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print\n",
    "transaction['Fruit Size Code'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction['Fruit Size Code']=transaction['Fruit Size Code'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(transaction['Fruit Size Code'].unique()))\n",
    "transaction['Fruit Size Code'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3) Dataset - Dispatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_empty_column(dispatched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_unilable_column(dispatched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Convert to Time Series Data\n",
    "Some of the attributes in this dataset contain datetime information,i.e. \n",
    "- Ok Until Date\n",
    "- Pack Date\n",
    "- Transaction Date\n",
    "- Transaction Date Time\n",
    "\n",
    "Since the focus of this project is on pallet transaction, this dataset will be converted into time series data based on Transaction Date Time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction['Transaction Date Time']=pd.to_datetime(transaction['Transaction Date Time'],format='%d/%m/%Y %I:%M:%S %p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction.set_index(keys='Transaction Date Time',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1) Transaction\n",
    "    7.1.1) Drop duplicated transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_drop_duplicate=transaction.shape\n",
    "transaction.drop_duplicates(inplace=True)\n",
    "print('Total number of transaction before dropping duplicate : {}'.format(before_drop_duplicate[0]))\n",
    "print('Total number of transaction after dropping duplicate : {}'.format(transaction.shape[0]))\n",
    "print('Total number of transaction decrease by {:.2f} %'.format(\n",
    "    (before_drop_duplicate[0]-transaction.shape[0])/before_drop_duplicate[0]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_drop_duplicate2=dispatched.shape\n",
    "dispatched.drop_duplicates(inplace=True)\n",
    "print('Total number of transaction before dropping duplicate : {}'.format(before_drop_duplicate2[0]))\n",
    "print('Total number of transaction after dropping duplicate : {}'.format(dispatched.shape[0]))\n",
    "print('Total number of transaction decrease by {:.2f} %'.format(\n",
    "    (before_drop_duplicate2[0]-dispatched.shape[0])/before_drop_duplicate2[0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2) Dispatched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the attributes in this dataset contain datetime information,i.e.\n",
    "\n",
    "    - Ok Until Date\n",
    "    - Pack Date\n",
    "    - Loadout Date\n",
    "    - Load Start Date\n",
    "\n",
    "Since the focus of this project is on loadout bay, this dataset will be converted into time series data based on Loadout Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispatched['Loadout Date']=pd.to_datetime(dispatched['Loadout Date'],format='%d/%m/%Y %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispatched.sort_values(by=['Loadout Date','Order Number','Envelope Number'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispatched.set_index(keys='Loadout Date',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.style.available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Daily_Transaction=transaction['Pallet Number'].resample('D').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure()\n",
    "axes=fig.add_axes([0,0,6,4])\n",
    "Daily_Transaction.plot(marker='o',ax=axes,fontsize=50,markersize=25)\n",
    "axes.set_title('Daily Pallet Transaction',fontsize=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derive Attribute - Shift, Shift_Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction['Day_Shift']=(transaction.index.hour>=7) & (transaction.index.hour<19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction['Shift_Hour']=transaction.index.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction['Shift_Date']=transaction.index.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction['Shift_Date']=pd.to_datetime(transaction['Shift_Date'],format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle shift date of night shift with transaction time over 12am\n",
    "transaction.loc[transaction['Shift_Hour']<7,'Shift_Date']=transaction.loc[transaction['Shift_Hour']<7,'Shift_Date']+datetime.timedelta(days=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction.loc[:,'Shift_Date'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Daily Shift Transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_shift_transaction=transaction.groupby(['Shift_Date','Day_Shift'])['Pallet Number'].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_shift_transaction['Day_Shift']=daily_shift_transaction['Day_Shift'].map({True:'Dayshift',False:'Nightshift'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_daily_shift_transaction=pd.pivot(data=daily_shift_transaction,index='Shift_Date',columns='Day_Shift',values='Pallet Number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_daily_shift_transaction['Allshift']=pv_daily_shift_transaction.sum(axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(pv_daily_shift_transaction['Dayshift'],label='Dayshift',marker='o',color='blue')\n",
    "plt.plot(pv_daily_shift_transaction['Nightshift'],label='Nightshift',marker='X',markersize=10,color='black')\n",
    "pv_daily_shift_transaction['Allshift'].plot.area(alpha=0.3,color='skyblue')\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Transaction')\n",
    "plt.title('Daily Shift Transaction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(15,6))\n",
    "axes=fig.add_axes([0,0,6,4])\n",
    "transaction['New Value'].value_counts(dropna=False)[:15].plot(kind='bar',ax=axes,fontsize=60)\n",
    "axes.set_title('Pallet New Location',fontsize=80)\n",
    "axes.set_xlabel('New Location',fontsize=80)\n",
    "axes.set_ylabel('Transaction',fontsize=80)\n",
    "axes.set_ylim([0,7000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['New Value']=='CONTR'\n",
    "t_CONTR=transaction.loc[transaction['New Value']=='CONTR',:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_CONTR.groupby(t_CONTR.index.day)['Pallet Number'].count().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_loadout=dispatched['Pallet Number'].resample('D').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_loadout['July 2019'].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispatched['Location Room Code'].value_counts(normalize=True,dropna=False).head().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_loadout['July 2019'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pallet in Both Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pallet_in_transaction=transaction['Pallet Number'].unique()\n",
    "len(pallet_in_transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pallet_in_dispatched=dispatched['Pallet Number'].unique()\n",
    "len(pallet_in_dispatched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pallet_in_both=[c for c in pallet_in_dispatched if c in pallet_in_transaction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pallet_in_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Row with UNKNOWN value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction[transaction['Pallet Number']==59699489][['Pallet Number','Transaction Date','Username','Previous Value','New Value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
