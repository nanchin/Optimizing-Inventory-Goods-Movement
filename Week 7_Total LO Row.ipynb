{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updates:\n",
    "- 24 Oct: Start\n",
    "    - Shows Envelope Number is the most granular order level\n",
    "    - Shows more than 90 % of Shipments conducted during 6am - 6pm\n",
    "    - Shows More than 90% of Envelopes being shipped out on single Loadout Time\n",
    "    - filter out envelope with total pallets less than 12, which occupy less than half of  the row\n",
    "    - Derived LO Rows needed by each envelope \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "\n",
    "from scipy.stats import skew,kurtosis\n",
    "import datetime\n",
    "import re\n",
    "import fnmatch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# to view all columns\n",
    "pd.set_option('display.max_columns',500)\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_object(arg_df):\n",
    "    \n",
    "    object_list=[]\n",
    "    category_list=[]\n",
    "    bool_list=[]\n",
    "    unilabel_list=[]\n",
    "    missing_list=[]\n",
    "    \n",
    "    for c in arg_df.columns:\n",
    "        if arg_df[c].dtypes==object:\n",
    "            object_list.append(c)\n",
    "        elif str(arg_df[c].dtypes)=='category':\n",
    "            category_list.append(c)\n",
    "        elif arg_df[c].dtypes==bool:\n",
    "            bool_list.append(c)\n",
    "    if len(object_list)+len(category_list)+len(bool_list)>0:    \n",
    "        index_list=['Count','Unique','Missing (%)','Top','Top (%)','Bottom','Bottom (%)']\n",
    "        df_summary=pd.DataFrame(data=np.zeros((len(index_list),len(object_list))),index=index_list,columns=object_list)\n",
    "\n",
    "        for col in object_list+category_list+bool_list:\n",
    "            vc=arg_df[col].value_counts().reset_index()\n",
    "            df_summary.loc['Count',col]=(arg_df[col].count())\n",
    "            df_summary.loc['Unique',col]=len(arg_df[col].unique())\n",
    "            df_summary.loc['Missing (%)',col]=arg_df[col].isna().mean()*100\n",
    "            df_summary.loc['Top',col]=vc.iloc[0,0]\n",
    "            df_summary.loc['Top (%)',col]=vc.iloc[0,1]/len(arg_df)*100\n",
    "            if len(arg_df[col].unique())>2:\n",
    "                df_summary.loc['Bottom',col]=vc.iloc[-1,0]\n",
    "                df_summary.loc['Bottom (%)',col]=vc.iloc[-1,1]/len(arg_df)*100\n",
    "            elif (len(arg_df[col].unique())==2) & (df_summary.loc['Missing (%)',col]==0):\n",
    "                df_summary.loc['Bottom',col]=vc.iloc[-1,0]\n",
    "                df_summary.loc['Bottom (%)',col]=vc.iloc[-1,1]/len(arg_df)*100\n",
    "            else:\n",
    "                unilabel_list.append(col)\n",
    "            if df_summary.loc['Missing (%)',col]==100:\n",
    "                missing_list.append(col)\n",
    "                \n",
    "        df_summary=df_summary.T.sort_values(['Missing (%)','Unique'],ascending=False)\n",
    "        df_summary=df_summary[(df_summary['Unique']>1) & (df_summary['Missing (%)']!=100)]\n",
    "        df_summary.reset_index(inplace=True)\n",
    "        df_summary.index=df_summary.index+1\n",
    "        df_summary.columns=['Attribute']+index_list\n",
    "        \n",
    "        print('SUMMARY OF {} NON-NUMERICAL ATTRIBUTES:\\n'.format(\n",
    "            len(object_list)+len(category_list)+len(bool_list)))\n",
    "        if len(object_list)>0:\n",
    "            print('{} Object Columns'.format(len(object_list)))\n",
    "        if len(category_list)>0:\n",
    "            print('{} Categorical Columns'.format(len(category_list)))\n",
    "        if len(bool_list)>0:\n",
    "            print('{} Bool Columns'.format(len(bool_list)))\n",
    "        if len(unilabel_list)>0:\n",
    "            print('\\n{} Columns with Single Label : \\n{}'.format(len(unilabel_list),unilabel_list))\n",
    "        if len(missing_list)>0:\n",
    "            print('\\n{} Empty Columns: \\n{}'.format(len(missing_list),missing_list))   \n",
    "            \n",
    "        del arg_df,object_list,vc,index_list,unilabel_list,missing_list\n",
    "        gc.collect()\n",
    "        return df_summary\n",
    "    else:\n",
    "        print('No Non-Numerical Attributes')\n",
    "'============================================================='        \n",
    "def summary_numerical(arg_df):\n",
    "\n",
    "    target_list=[]\n",
    "    missing_list=[]\n",
    "    zero_skew_list=[]\n",
    "    \n",
    "    for c in arg_df.columns:\n",
    "        datatype=arg_df[c].dtypes\n",
    "        if datatype != object and datatype != bool and str(datatype) != 'category' and str(datatype) !='datetime64[ns]':\n",
    "            target_list.append(c)\n",
    "    if len(target_list)>0:\n",
    "        from scipy.stats import skew,kurtosis\n",
    "        \n",
    "        index_list=['Count','Missing (%)','Mean','Median','Min','Max','Skewness','Kurtosis']\n",
    "        df_summary=pd.DataFrame(data=np.zeros((len(index_list),len(target_list))),\n",
    "                                index=index_list,columns=target_list)\n",
    "        for col in target_list:\n",
    "            df_summary.loc['Count',col]=arg_df[col].count()\n",
    "            df_summary.loc['Missing (%)',col]=arg_df[col].isna().mean()*100\n",
    "            if df_summary.loc['Missing (%)',col]!=100:\n",
    "                df_summary.loc['Mean',col]=arg_df[col].mean()\n",
    "                df_summary.loc['Median',col]=arg_df[col].median()\n",
    "                df_summary.loc['Min',col]=arg_df[col].min()\n",
    "                df_summary.loc['Max',col]=arg_df[col].max()\n",
    "                df_summary.loc['Skewness',col]=skew(arg_df[col])\n",
    "                if df_summary.loc['Skewness',col]==0:\n",
    "                    zero_skew_list.append(col)\n",
    "                df_summary.loc['Kurtosis',col]=kurtosis(arg_df[col])\n",
    "            else:\n",
    "                missing_list.append(col)\n",
    "                \n",
    "        df_summary=df_summary.T.sort_values(['Missing (%)','Skewness'],ascending=False)\n",
    "        df_summary=df_summary[(df_summary['Skewness']!=0) & (df_summary['Missing (%)']!=100)]\n",
    "        df_summary.reset_index(inplace=True)\n",
    "        df_summary.index=df_summary.index+1\n",
    "        df_summary.columns=['Attribute']+index_list\n",
    "        \n",
    "        print('SUMMARY OF {} NUMERICAL ATTRIBUTES:'.format(len(target_list)))\n",
    "        if len(zero_skew_list)>0:\n",
    "            print('\\n{} Columns with Single Value: \\n{}'.format(len(zero_skew_list),zero_skew_list))\n",
    "        if len(missing_list)>0:\n",
    "            print('\\n{} Empty Columns: \\n{}'.format(len(missing_list),missing_list))\n",
    "        del arg_df,target_list,index_list\n",
    "        gc.collect()\n",
    "\n",
    "        return df_summary\n",
    "    else:\n",
    "        print('No Numerical Attributes')\n",
    "'==================================================================='        \n",
    "def drop_unilable_column(arg_df):\n",
    "    \n",
    "    target_list=[]\n",
    "    object_list=[]\n",
    "    number_list=[]\n",
    "    for c in arg_df.columns:\n",
    "        if (arg_df[c].dtypes==object) | (str(arg_df[c].dtypes)=='category') | (arg_df[c].dtypes==bool):\n",
    "            object_list.append(c)\n",
    "        elif str(arg_df[c].dtypes)!='datetime64[ns]':\n",
    "            number_list.append(c)\n",
    "    if len(object_list)>0:    \n",
    "        for c in object_list:\n",
    "            if len(arg_df[c].unique())==1:\n",
    "                target_list.append(c)\n",
    "            elif (len(arg_df[c].unique())==2) & (arg_df[c].isna().mean()>0):\n",
    "                target_list.append(c)\n",
    "    \n",
    "    if len(number_list)>0:   \n",
    "        from scipy.stats import skew\n",
    "        for c in number_list:\n",
    "            if skew(arg_df[c])==0:\n",
    "                target_list.append(c)\n",
    "                \n",
    "    if len(target_list)>0:\n",
    "        arg_df.drop(columns=target_list,axis='columns',inplace=True)\n",
    "        print('Drop {} Columns with Single Label:\\n{}'.format(len(target_list),target_list))\n",
    "    else: \n",
    "        print('No Columns with Single Label/Value')\n",
    "\n",
    "    del target_list,object_list\n",
    "'===================================================================' \n",
    "def drop_empty_column(arg_df):\n",
    "    target_list=[]\n",
    "    for c in arg_df.columns:\n",
    "        if arg_df[c].count()==0:\n",
    "            target_list.append(c)\n",
    "    if len(target_list)>0:\n",
    "        arg_df.drop(columns=target_list,axis=1,inplace=True)\n",
    "        print('Delete {} Empty Column : \\n{}'.format(len(target_list),target_list))\n",
    "    else:\n",
    "        print('No Empty Column')\n",
    "'==================================================================='        \n",
    "def drop_columns(arg_df,column_names):\n",
    "    arg_df.drop(columns=column_names,axis='columns',inplace=True)\n",
    "    print('Drop {} columns : \\n{}'.format(len(column_names),column_names))\n",
    "'============================================================='        \n",
    "def extract_room_row(arg_df,col_position):\n",
    "    '''To return unique Room-Row from standard Room-Row-Column-Height position data'''\n",
    "    roomrow=[]\n",
    "    roomrow=arg_df[col_position].apply(lambda x :x.split('-')[0]+'-'+x.split('-')[1] if '-' in x else x)\n",
    "    roomrow=roomrow.unique().tolist()\n",
    "    roomrow=pd.DataFrame(roomrow,columns=['Unique_Row']).sort_values(by='Unique_Row')\n",
    "    return roomrow['Unique_Row'].values\n",
    "'============================================================='  \n",
    "def convert_room_row(position):\n",
    "    if '-' in position:\n",
    "        w,x,y,z=position.split('-')\n",
    "        return w+'-'+x\n",
    "    else:\n",
    "        return position\n",
    "'============================================================='  \n",
    "def generate_dif_columns(arg_df,left_column,left_sffx,right_sffx):\n",
    "    common_title=[]\n",
    "    for idx,c in enumerate(left_column):\n",
    "        common_title.append(left_column[idx].split(left_sffx)[0])\n",
    "    print('There are {} common columns : \\n{}'.format(len(left_column),common_title))\n",
    "\n",
    "    for idx,c in enumerate(common_title):\n",
    "        compare_col=common_title[idx].replace(' ','')\n",
    "        arg_df['dif_'+compare_col]=(arg_df[c+left_sffx]!=arg_df[c+right_sffx]) & (arg_df[c+right_sffx].notna())\n",
    "    print('\\nColumns Generated : {}'.format(len(common_title)))\n",
    "'==================================================================='        \n",
    "def find_time_dif_hour(arg_df,ref_date,proc_date):\n",
    "    new_date_attribute=[c.replace(' ','_') for c in proc_date]\n",
    "    for idx,c in enumerate(proc_date):\n",
    "        arg_df[new_date_attribute[idx]+'_hour']=arg_df[c]-arg_df[ref_date]\n",
    "        arg_df[new_date_attribute[idx]+'_hour']=arg_df[new_date_attribute[idx]+'_hour'].astype('timedelta64[h]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Import 2 dataset : trans and disp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='C:/Users/Nan/Documents/GitHub_Data/'\n",
    "file_1=path+'p_transaction_2.csv'\n",
    "file_2=path+'p_dispatched_2.csv'\n",
    "\n",
    "filename_1=file_1\n",
    "filename_2=file_2\n",
    "\n",
    "sffx_transaction='_Trsc'\n",
    "sffx_dispatched='_Dptch'\n",
    "target_process='FW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans=pd.read_csv(filename_1)\n",
    "disp=pd.read_csv(filename_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing so that common columns have same set of data label\n",
    "    -change to string:Fruit Size Code_Trsc\n",
    "    -truncate 0 at first position:Purchase Pool Code_Trsc\n",
    "    -incorrect calculation as referring to dif,should remain in datetime: date columns(incl Ok_Until_Date_day,Pack_Date_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans['Fruit Size Code']=trans['Fruit Size Code'].astype('str')\n",
    "disp['Fruit Size Code']=disp['Fruit Size Code'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans['Purchase Pool Code']=trans['Purchase Pool Code'].apply(lambda x:x.split('0')[1] if x[0]=='0' else x)\n",
    "disp['Purchase Pool Code']=disp['Purchase Pool Code'].apply(lambda x:x.split('0')[1] if x[0]=='0' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop_columns(disp,['Pack_Date_day', 'Ok_Until_Date_day'])\n",
    "drop_columns(disp,['Pack_Date_day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to prevent OK Until Date with same date but dif time\n",
    "trans['Ok Until Date']=pd.to_datetime(trans['Ok Until Date'],format='%Y-%m-%d %H:%M:%S')\n",
    "disp['Ok Until Date']=pd.to_datetime(disp['Ok Until Date'],format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans['Ok Until Date']=trans['Ok Until Date'].dt.date\n",
    "disp['Ok Until Date']=disp['Ok Until Date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans['Transaction Date Time']=pd.to_datetime(trans['Transaction Date Time'],format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp['Location Height'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp['Location Column']=disp['Location Column'].map(lambda x:x[:-2] if x!='na' else x)\n",
    "disp['Location Height']=disp['Location Height'].map(lambda x:x[:-2] if x!='na' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disp=disp[['Loadout Date','Load Start Date','Pallet Number','Order Number','Envelope Number','Container','Container Number',\n",
    "                'Location Room Code','Location Row Code','Location Column',\n",
    "                'Location Height','Loadout Priority','Shipment Type Code','Destination Port Code',\n",
    "               'Trucking Company Code','Stacking Configuration Code','Pack Type Code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive Last Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disp.loc[df_disp['Location Column']!='na']['Location Room Code'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=df_disp['Location Room Code'].map(lambda x:x.startswith('Q'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disp.loc[criterion,'Last Value']=df_disp['Location Room Code']+'-'+df_disp['Location Row Code']+'-'+df_disp['Location Column']+'-'+df_disp['Location Height']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disp.loc[df_disp['Last Value'].isna(),'Last Value']=df_disp['Location Room Code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disp['RoomRow_Last Value']=df_disp['Last Value'].map(convert_room_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To investigate the distribution of loadoat time\n",
    "- Findings : 96.74 % of Shipments conducted during 6am - 6pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disp['Loadout Date']=pd.to_datetime(df_disp['Loadout Date'],format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disp['Load Hour']=df_disp['Loadout Date'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disp['Load Hour'].plot(kind='hist',bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of Shipments conducted during day shift is : {:.2f} %'.\n",
    "      format(len(df_disp[(df_disp['Load Hour']>=6) & (df_disp['Load Hour']<=18)])/len(df_disp)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate Relationship of  _Order Number_,_Container Number_ and _Envelope Number_ \n",
    "- Findings : Envelope Number indicate the most granular level of order structure\n",
    "- There are two types of order, container order and chartered order\n",
    "- Container order contains envelope number and container number\n",
    "- Chartered order contains envelope number only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df=df_disp.groupby(['Order Number','Container Number','Envelope Number'])['Pallet Number'].count()\n",
    "temp_df=temp_df.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df[88:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate Total Number of Shipment Date per Envelope\n",
    "- Findings : More than 90% of Envelopes being shipped out on single Loadout Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp_df=df_disp.groupby(['Envelope Number','Loadout Date'])['Pallet Number'].count()\n",
    "temp_df=temp_df.to_frame().reset_index()\n",
    "total_shipment_per_envelope=temp_df['Envelope Number'].value_counts().to_frame().reset_index()\n",
    "print('Envelope that shipped on single shipment : {:.2f} %'.format(\n",
    "len(total_shipment_per_envelope[total_shipment_per_envelope['Envelope Number']==1])/len(total_shipment_per_envelope)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the delivery Date of Order \n",
    "- Findings : The Loadout Time of envelopes that belongs to same order number could be different although all of the envelopes RTG at same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df=df_disp.groupby(['Order Number','Envelope Number','Loadout Date'])['Pallet Number'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_disp[df_disp['Envelope Number']==199288]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_disp.groupby('Envelope Number')['RoomRow_Last Value'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_disp.loc[df_disp['Envelope Number']==199316,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hightop pallets also being placed in normal LO Room\n",
    "df_disp[df_disp['Stacking Configuration Code']!='N']['RoomRow_Last Value'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Total Pallet of all dispatched Envelope\n",
    "    - More than 50 % of order belongs to Container order of 20 pallets \n",
    "    - followed by chartered order of 24 pallets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disp.groupby('Envelope Number')['Pallet Number'].count().plot(kind='hist',bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=df_disp.groupby('Envelope Number')['Pallet Number'].count().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of Orders with 20 pallet = {:.2f} %'.format(len(temp[temp['Pallet Number']==20])/len(temp)))\n",
    "print('Percentage of Orders with 24 pallet = {:.2f} %'.format(len(temp[temp['Pallet Number']==24])/len(temp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out Envelope with Total Pallets  of less than 12 pallets, which will occupied at least half of the row when double-stacked\n",
    "    - data  july 2019 : will filter out 10 % of envelopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_pallet=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp=df_disp.groupby('Envelope Number')['Pallet Number'].count().to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Quantity of Envelope of Original Dataset = {}'.format(temp.shape[0]))\n",
    "print('Quantity of Envelope of Filtered Dataset = {}'.format(temp[temp['Pallet Number']>=min_pallet].shape[0]))\n",
    "print('Reduction % = {:.4f} %'.format((temp.shape[0]-temp[temp['Pallet Number']>=min_pallet].shape[0])/temp.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp=temp.loc[temp['Pallet Number']>=min_pallet,'Envelope Number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disp=df_disp.merge(temp,how='inner',on='Envelope Number')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive isHighTops so that stack type 'L' and 'N' can be grouped together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disp['Stacking Configuration Code'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disp.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_disp['isHighTops']=df_disp['Stacking Configuration Code'].isin(['1','2','3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derive LO Row needed for all envelopes\n",
    "    - Rule : Hightops cannot be double stacked in LO Room\n",
    "    - Rule : MB packstyle can be double stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this shows MB pallet can be double-stacked\n",
    "print(trans[trans['Pack Style Code'].isin(['MB'])]['Previous_Height'].value_counts())\n",
    "\n",
    "# although the result shows hightops is being double-stacked, it is violating the rules especially in LO ROom\n",
    "print(trans[~(trans['Stacking Configuration Code'].isin(['N']))]['Previous_Height'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=df_disp.groupby(['Envelope Number','isHighTops'])['Pallet Number'].count().to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive 'Cells Needed' that handle business rule of all pallets can be double-stacked except HighTops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "temp.loc[temp['isHighTops']==False,'Cells Needed']=temp['Pallet Number']/2\n",
    "temp.loc[temp['isHighTops']==True,'Cells Needed']=temp['Pallet Number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp['Cells Needed']=temp['Cells Needed'].map(lambda x:math.ceil(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_disp_cells=temp.groupby('Envelope Number')['Cells Needed'].sum().to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disp_cells.rename(index=str,columns={'Cells Needed':'Total Cells'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disp_cells['Total Cells'].plot(kind='Hist',bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_disp_cells['Total Cells'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive 'Row Needed' by dividing 'Total Cells' by 12 and round up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disp_cells['Row Needed']=df_disp_cells['Total Cells']/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disp_cells.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_disp_cells['Row Needed']=df_disp_cells['Row Needed'].map(lambda x:math.ceil(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disp_cells['Row Needed'].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disp_cells['Row Needed'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge with Trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge disp_temp with trans_temp to filter out pallet number in common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_temp=trans[['Transaction Date Time','Pallet Number','New Value','Previous Value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Transactions Line : {}'.format(trans_temp.shape[0]))\n",
    "print('Total Number of Pallet : {} '.format(len(trans_temp['Pallet Number'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_merge=trans_temp.merge(df_disp['Pallet Number'],how='inner',on='Pallet Number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Transactions Line : {}'.format(trans_merge.shape[0]))\n",
    "print('Total Number of Pallet : {} '.format(len(trans_merge['Pallet Number'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_last=trans_merge.groupby('Pallet Number')[['New Value','Previous Value','Transaction Date Time']].last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trans_last.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_last.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge disp_temp with trans_last to populate last position in trans with disp_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_merge=df_disp.merge(trans_last,how='left',on='Pallet Number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of dispatched pallet with no trans record : {}'.format(len(disp_merge[disp_merge['New Value'].isna()])))\n",
    "print('Total number of dispatched pallet with trans record : {}'.format(len(disp_merge[disp_merge['New Value'].notna()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out data with last position in trans is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_merge=disp_merge.loc[disp_merge['New Value'].notna()]\n",
    "disp_merge.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_merge.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_simple=disp_merge[['Loadout Date','Load Start Date','Pallet Number','Last Value','New Value','Previous Value','Transaction Date Time','Order Number','Envelope Number','Container Number','Stacking Configuration Code','Pack Type Code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_simple['RoomRow_New Value']=disp_simple['New Value'].map(convert_room_row)\n",
    "disp_simple['RoomRow_Previous Value']=disp_simple['Previous Value'].map(convert_room_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does Pallets that belongs to same Envelope Number being taken from same rows prior to shipment ?\n",
    "    - part of packhouse SOP\n",
    "    - hard to extract this part of information from data as the LO Row info could be in any of last few transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=disp_simple.groupby(['Envelope Number','RoomRow_Previous Value'])['Pallet Number'].count().to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp[100:120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - more than 80 % of pallet dispatched have Last Value = New Value\n",
    "    - hightop pallets also being placed in normal LO Room"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more than 80 % of transaction have Last Value = New Value\n",
    "(disp_simple['Last Value']==disp_simple['New Value']).sum()/len(disp_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_simple[disp_simple['Last Value']!=disp_simple['New Value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
